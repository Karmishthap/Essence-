{"cells":[{"cell_type":"markdown","metadata":{"id":"5AvNU5BgMs1H"},"source":["# **REPORT**"]},{"cell_type":"markdown","metadata":{"id":"w572AcVJIfuO"},"source":["**PRE_REQUISITES**\n","\n","i)Python class with a singular interface== creating a **consistent** way to interact with different models\n","\n","ii)**Common method :Predict() method**\n","\n","\n","\n","\n","\n","iii)**Baseclass == EmbeddingModel **\n","\n","*   to make changes and improvements to common functioning\n","*   It provides a consistent interface (predict()) that can be used with various models\n","*   making it easier to switch between them.\n","\n","\n","\n","iv)**super().__init__():** Calls the constructor (__init__ method) of the superclass. This is typically used in the constructor of a subclass to initialize attributes defined in the superclass.(for reffering to the parent class )\n","\n","\n","#EmbeddingModel\n","\n","\n","1.   _init__(self): The constructor method initializes the object of the class. In this case, it sets the trained attribute to False.\n","\n","*   When a new instance of EmbeddingModel is created, it's marked as not trained (False) until the model is explicitly trained.\n","2. train(self, df): This method is intended to be overridden by subclasses. It takes a DataFrame (df) as input and is meant for training an embedding model. In the base class, it does nothing.\n","\n","3. predict(self, df): This method is intended to be overridden by subclasses. It takes a DataFrame (df) as input and is meant for making predictions using the trained embedding model. In the base class, it does nothing.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"O1t1zVWkUcRP"},"source":["#Models Used\n","\n","\n","1.   BERT\n","2.   ELMO\n","3.   USE\n","4.   DOC2VEC\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZbzIrr6SU4Tj"},"source":["#Comparitive Analysis(With VS Without Encapsulation)\n","\n","> Indented block\n","\n","\n","\n","A.**BERT**:\n","\n","\n","* Without encapsulation:\n","1.  Accuracy: 91.67%\n","2.   Embedding Time: 80.5545768737793 seconds\n","3. Training Time: 0.5687379837036133 seconds\n","Prediction Time: 0.0012612342834472656 seconds\n","\n","\n","* With encapsulation\n","1. Accuracy (BERT): 96.93%\n","2. Embedding Time (BERT): 108.97 seconds\n","3. Training Time (BERT): 0.27 seconds\n","4. Prediction Time (BERT): 95.33 seconds\n","\n","B.**ELMO**:\n","\n","\n","*   Without encapsulation:\n","\n","1. Accuracy: 97.92%\n","2. Embedding Time: 268.30 seconds\n","3. Training Time: 0.11 seconds\n","4. Prediction Time: 0.00 seconds\n","\n","\n","\n","*   With encapsulation\n","1. Accuracy (ELMo): 99.44%\n","2. Embedding Time (ELMo): 357.49 seconds\n","3. Training Time (ELMo): 0.11 seconds\n","4. Prediction Time (ELMo): 314.30 seconds\n","\n","\n","C.**USE**:\n","\n","\n","*   Without encapsulation:\n","1. Accuracy: 90.28%\n","2. Embedding Time: 3.31 seconds\n","3. Training Time: 0.03 seconds\n","4. Prediction Time: 0.00 seconds\n","\n","*   With encapsulation\n","1. Accuracy (USE): 87.43%\n","2. Embedding Time (USE): 3.69 seconds\n","3. Training Time (USE): 0.05 seconds\n","4. Prediction Time (USE): 3.08 seconds\n","\n","D.**DOC2VEC**:\n","\n","\n","*   Without encapsulation:\n","1. Accuracy (Doc2Vec): 54.86%\n","2. Embedding Time (Doc2Vec): 0.59 seconds\n","3. Training Time (Doc2Vec): 0.03 seconds\n","4. Prediction Time (Doc2Vec): 0.00 seconds\n","\n","*   With encapsulation\n","1. Accuracy (Doc2Vec): 56.01%\n","2. Embedding Time (Doc2Vec): 2.15 seconds\n","3. Training Time (Doc2Vec): 0.02 seconds\n","4. Prediction Time (Doc2Vec): 0.68 seconds\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7Gf4UGHLZevk"},"source":["#Inference(With VS Without Encapsulation)\n","\n","\n","**General Inference:**\n","\n","1. BERT:\n","\n","Without Encapsulation: Shows a high accuracy but long embedding and prediction times.\n","With Encapsulation: Improved accuracy but significantly longer embedding and prediction times.\n","\n","2. ELMo:\n","\n","Without Encapsulation: High accuracy with relatively long embedding time.\n","With Encapsulation: Further improved accuracy, longer embedding time, and significantly longer prediction time.\n","\n","3. USE:\n","\n","Without Encapsulation: Decent accuracy with fast embedding and prediction times.\n","With Encapsulation: Slight decrease in accuracy, slightly increased embedding time, and longer prediction time.\n","Doc2Vec:\n","\n","Without Encapsulation: Low accuracy with fast embedding and prediction times.\n","With Encapsulation: Slightly improved accuracy, increased embedding time, and longer prediction time.\n","\n","\n","\n","**Key Observations:**\n","\n","1.Accuracy:\n","\n","Generally, encapsulation improves accuracy, suggesting a more robust model, but the extent varies.\n","\n","2.Embedding Time:\n","\n","Encapsulation tends to increase the embedding time for all models.\n","ELMo and BERT, being more complex models, show a more significant increase.\n","\n","3.Training Time:\n","\n","Training time is generally low for all models.\n","Encapsulation doesn't significantly impact training time.\n","\n","\n","4.Prediction Time:\n","\n","Encapsulation often leads to longer prediction times, especially for complex models like ELMo and BERT.\n","\n","**Considerations:**\n","\n","1. Encapsulation provides a unified interface but may lead to increased computational costs.\n","\n","2. Trade-offs between accuracy improvements and increased computational time should be considered based on specific use-case requirements.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"K9r1BAA9IPhJ"},"source":["#Challenges and how F1 Scores help in rectifying it\n","\n","1)**Imbalanced Classes:** If your dataset has imbalanced classes (i.e., one class has significantly more samples than the other), accuracy may not be a reliable metric. F1 score considers both false positives and false negatives and is more robust in such situations.\n","\n","2)**Class Distribution Changes:** If your model is sensitive to changes in the distribution of classes, accuracy might not capture these changes effectively. F1 score can help highlight issues related to precision and recall.\n","\n","3)**False Positives vs. False Negatives:** In some applications, false positives and false negatives have different consequences. F1 score provides a balance between precision and recall, allowing you to assess the trade-off between these two types of errors.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pRuY1Zd-KmSY"},"source":["#Inference(With Vs Without F1 Score )\n","\n","\n","A.BERT:\n","1. High accuracy and F1 score.\n","Strong contextual embeddings capture intricate relationships.\n","\n","***Disadvantages:***\n","\n","2. High computational cost for both embedding and prediction.\n","Complex model architecture can be resource-intensive.\n","\n","***Improvements:***\n","\n","Consider using smaller BERT variants (e.g., DistilBERT) for faster inference.\n","\n","\n","B.ELMo:\n","\n","***Advantages:***\n","\n","1. Exceptional accuracy and F1 score.\n","Captures complex linguistic features with deep contextual embeddings.\n","\n","***Disadvantages:***\n","\n","1. Extremely high embedding and prediction times.\n","Large model size can be challenging for deployment.\n","\n","***Improvements:***\n","\n","Use ELMo variants with reduced dimensions for faster processing.\n","\n","\n","C.USE (Universal Sentence Encoder):\n","\n","***Advantages:***\n","\n","1. Balanced accuracy with faster embedding and prediction times.\n","Versatility in handling various sentence structures.\n","\n","***Improvements:***\n","\n","1. Fine-tune on domain-specific data for enhanced accuracy.\n","\n","\n","D.Doc2Vec:\n","\n","***Advantages:***\n","\n","Fast processing times for both embedding and prediction.\n","Simplicity and efficiency for various text data.\n","***Disadvantages:***\n","\n","Lower accuracy compared to contextual embeddings.\n","\n","***Improvements:***\n","\n","Explore hyperparameter tuning for improved accuracy.\n","Consider training for additional epochs for better embeddings.\n","\n","#Overall Recommendations:\n","\n","If computational resources are not a constraint, and the highest accuracy is crucial, ELMo or BERT might be preferred.\n","For a balance between accuracy and efficiency, Universal Sentence Encoder (USE) could be a suitable choice.\n","If computational efficiency is a priority and sacrificing some accuracy is acceptable, Doc2Vec is a feasible option."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3445,"status":"ok","timestamp":1712080634911,"user":{"displayName":"Karmishtha Patnaik","userId":"00266511564033128500"},"user_tz":-330},"id":"VYQIbzuGG5zV","outputId":"2dd1a9ab-d59c-4fdf-d3ac-3f33b5d2f936"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"hQ0HKYYNIEkV"},"source":["# **ELMO DEPENDENCIES**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6vmDEhZGf-O"},"outputs":[],"source":["pip install tensorflow==2.15.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hWh7rWTBGnzV"},"outputs":[],"source":["pip install allennlp"]},{"cell_type":"markdown","metadata":{"id":"PavxdLBFIMTl"},"source":["# **DOC2VEC DEPENDENCIES**"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1663,"status":"ok","timestamp":1712080731604,"user":{"displayName":"Karmishtha Patnaik","userId":"00266511564033128500"},"user_tz":-330},"id":"ZK8tg97MI1FI","outputId":"999706ff-0c8d-4309-ea9a-29e4ce240a22"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{"id":"rePZXCPkeEDR"},"source":["TinyBERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MMFgsWwahGm"},"outputs":[],"source":["!pip install -U sentence-transformers"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6888,"status":"ok","timestamp":1712080795650,"user":{"displayName":"Karmishtha Patnaik","userId":"00266511564033128500"},"user_tz":-330},"id":"GenM6Jzvalgf"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModel\n","from sentence_transformers import SentenceTransformer\n","import torch\n","import time"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":423,"status":"ok","timestamp":1712080803515,"user":{"displayName":"Karmishtha Patnaik","userId":"00266511564033128500"},"user_tz":-330},"id":"6qherz8nam10"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModel, AdamW\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from sentence_transformers import SentenceTransformer\n","from transformers import AutoTokenizer\n","from transformers import BertTokenizerFast\n","import torch\n","import time"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"IeFVH2hheGgB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712080812537,"user_tz":-330,"elapsed":6334,"user":{"displayName":"Karmishtha Patnaik","userId":"00266511564033128500"}},"outputId":"96c2f901-f871-482c-c2ce-d6a5edf1ccc8"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-TinyBERT-L6-v2')\n","model = AutoModel.from_pretrained('sentence-transformers/paraphrase-TinyBERT-L6-v2')"]},{"cell_type":"code","source":["rm -rf /root/.cache/huggingface"],"metadata":{"id":"feWfxxiemqbF","executionInfo":{"status":"ok","timestamp":1712080816097,"user_tz":-330,"elapsed":614,"user":{"displayName":"Karmishtha Patnaik","userId":"00266511564033128500"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModel\n","from sentence_transformers import SentenceTransformer, models\n","\n","# Load tokenizer and model separately\n","tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-TinyBERT-L6-v2')\n","model = AutoModel.from_pretrained('sentence-transformers/paraphrase-TinyBERT-L6-v2')\n","\n","# Define SentenceTransformer model with the loaded tokenizer and model\n","sentence_transformer_model = SentenceTransformer(modules=[models.Pooling(model.get_input_embeddings().embedding_dim)])\n"],"metadata":{"id":"RtoWBfDem702"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","\n","model = SentenceTransformer('sentence-transformers/paraphrase-TinyBERT-L6-v2')\n"],"metadata":{"id":"wdU4qyTcm-uc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"brfScnEleVL_","executionInfo":{"status":"ok","timestamp":1712080854083,"user_tz":-330,"elapsed":1232,"user":{"displayName":"Karmishtha Patnaik","userId":"00266511564033128500"}}},"outputs":[],"source":["model = SentenceTransformer('sentence-transformers/paraphrase-TinyBERT-L6-v2')"]},{"cell_type":"code","source":["import time\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, f1_score\n","import torch\n","import tensorflow_hub as hub\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from nltk.tokenize import word_tokenize\n","from transformers import BertTokenizer, BertModel\n","from allennlp.modules.elmo import Elmo, batch_to_ids\n","\n","\n","class EmbeddingModel:\n","    def __init__(self, embedding_model):\n","        self.trained = False\n","        self.classifier = LogisticRegression(max_iter=1000)\n","        self.embedding_model = embedding_model\n","\n","    def train(self, df):\n","        pass\n","\n","    def predict(self, df):\n","        pass\n","\n","    def sentence_embedding(self, task_name, task_type, task_intensity):\n","        pass\n","\n","    def print_results(self, accuracy, f1, training_time, prediction_time):\n","        print(f'{self.embedding_model}')\n","        print(f'Accuracy ({self.embedding_model}): {accuracy * 100:.2f}%')\n","        print(f'F1 Score ({self.embedding_model}): {f1:.2f}%')\n","        print(f'Training Time ({self.embedding_model}): {training_time:.2f} seconds')\n","        print(f'Prediction Time ({self.embedding_model}): {prediction_time:.2f} seconds')\n","        print()\n","\n","\n","class TinyBERTEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__('TinyBERT')\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        self.model = BertModel.from_pretrained('bert-base-uncased')\n","\n","    def sentence_embedding(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        inputs = self.tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n","        with torch.no_grad():\n","            outputs = self.model(**inputs)\n","        embedding = torch.mean(outputs.last_hidden_state, dim=1).squeeze().numpy()\n","        return [embedding.tolist()]  # Return embedding as a list of lists\n","\n","    def train(self, df):\n","        start_time = time.time()\n","        super().train(df)  # Call superclass train method\n","\n","        # Compute embeddings for all samples\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","\n","        # Reshape the embeddings to 2D array\n","        X_train = df['embedding'].to_list()\n","\n","        # Flatten the list of embeddings\n","        X_train = [item for sublist in X_train for item in sublist]\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        y_train = df['Generated']\n","        self.classifier = LogisticRegression(max_iter=1000)\n","        self.classifier.fit(X_train, y_train)\n","        self.trained = True\n","\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        super().predict(df)  # Call superclass predict method\n","\n","        # Compute embeddings for test samples\n","        X_test = df.apply(lambda row: self.sentence_embedding(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","\n","        # Flatten the embeddings\n","        X_test = [item for sublist in X_test for item in sublist]\n","\n","        # Make predictions\n","        y_pred = self.classifier.predict(X_test)\n","\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","\n","        # Compute metrics\n","        f1 = f1_score(df['Generated'], y_pred, average='weighted')\n","        accuracy = accuracy_score(df['Generated'], y_pred)\n","\n","        # Print results\n","        self.print_results(accuracy, f1, self.training_time, self.prediction_time)\n","\n","\n","class BERTEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__('BERT')\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        self.model = BertModel.from_pretrained('bert-base-uncased')\n","\n","    def sentence_embedding(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        tokens = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n","        with torch.no_grad():\n","            outputs = self.model(**tokens)\n","        last_hidden_states = outputs.last_hidden_state\n","        sentence_embedding = torch.mean(last_hidden_states, dim=1).squeeze()\n","        return sentence_embedding.numpy()\n","\n","    def train(self, df):\n","        super().train(df)  # Call superclass train method\n","        # Implementation of train method for BERT model\n","\n","    def predict(self, df):\n","        super().predict(df)  # Call superclass predict method\n","        # Implementation of predict method for BERT model\n","\n","\n","class ELMoEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__('ELMo')\n","        options_file = 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json'\n","        weight_file = 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'\n","        self.elmo = Elmo(options_file, weight_file, 1, dropout=0)\n","\n","    def sentence_embedding(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        character_ids = batch_to_ids([sentence.split()])  # assuming a single sentence\n","        embeddings = self.elmo(character_ids)\n","        sentence_embedding = torch.mean(embeddings['elmo_representations'][0], dim=1).squeeze()\n","        return sentence_embedding.detach().numpy()\n","\n","    def train(self, df):\n","        super().train(df)  # Call superclass train method\n","        # Implementation of train method for ELMo model\n","\n","    def predict(self, df):\n","        super().predict(df)  # Call superclass predict method\n","        # Implementation of predict method for ELMo model\n","\n","\n","class USEEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__('USE')\n","        self.use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n","\n","    def sentence_embedding(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        embedding = self.use_model([sentence])[0].numpy()\n","        return embedding\n","\n","    def train(self, df):\n","        super().train(df)  # Call superclass train method\n","        # Implementation of train method for USE model\n","\n","    def predict(self, df):\n","        super().predict(df)  # Call superclass predict method\n","        # Implementation of predict method for USE model\n","\n","\n","class Doc2VecEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__('Doc2Vec')\n","        self.doc2vec_model = Doc2Vec(vector_size=300, window=5, min_count=1, workers=4, epochs=20)\n","\n","    def preprocess_text(self, text):\n","        return word_tokenize(text.lower())\n","\n","    def sentence_embedding(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        tokenized_text = self.preprocess_text(sentence)\n","        embedding = self.doc2vec_model.infer_vector(tokenized_text)\n","        return embedding\n","\n","    def train(self, df):\n","        super().train(df)  # Call superclass train method\n","        # Implementation of train method for Doc2Vec model\n","\n","    def predict(self, df):\n","        super().predict(df)  # Call superclass predict method\n","        # Implementation of predict method for Doc2Vec model\n","\n","\n","# Instantiate and use the models\n","bert_model = BERTEmbeddingModel()\n","elmo_model = ELMoEmbeddingModel()\n","use_model = USEEmbeddingModel()\n","doc2vec_model = Doc2VecEmbeddingModel()\n","\n","# Load your CSV dataset\n","df = pd.read_csv('/content/drive/My Drive/Sem-4/Essence/dataset-20240111.csv')\n","\n","# Train and predict for each model\n","bert_model.train(df)\n","bert_model.predict(df)\n","\n","elmo_model.train(df)\n","elmo_model.predict(df)\n","\n","use_model.train(df)\n","use_model.predict(df)\n","\n","doc2vec_model.train(df)\n","doc2vec_model.predict(df)\n"],"metadata":{"id":"ARZqQYBCHF_y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#FIRST CHANGE::: Notimplementation ..."],"metadata":{"id":"G7R4Z7MIKGdP"}},{"cell_type":"code","source":["import time\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, f1_score\n","import torch\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from nltk.tokenize import word_tokenize\n","from transformers import BertTokenizer, BertModel\n","from allennlp.modules.elmo import Elmo, batch_to_ids\n","\n","class EmbeddingModel:\n","    def __init__(self, embedding_model):\n","        self.trained = False\n","        self.classifier= LogisticRegression(max_iter=1000)\n","        self.embedding_model = embedding_model\n","\n","    def train(self, df):\n","        raise NotImplementedError\n","\n","    def predict(self, df):\n","        raise NotImplementedError\n","\n","    def sentence_embedding(self, task_name, task_type, task_intensity):\n","        raise NotImplementedError\n","\n","class TinyBERTEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__('TinyBERT')\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        self.model = BertModel.from_pretrained('bert-base-uncased')\n","\n","    def sentence_embedding_tinybert(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        inputs = self.tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n","        with torch.no_grad():\n","            outputs = self.model(**inputs)\n","        embedding = torch.mean(outputs.last_hidden_state, dim=1).squeeze().numpy()\n","        return [embedding.tolist()]  # Return embedding as a list of lists\n","\n","# Update the train method of TinyBERTEmbeddingModel\n","    def train(self, df):\n","        start_time = time.time()\n","\n","        # Compute embeddings for all samples\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding_tinybert(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","\n","        # Reshape the embeddings to 2D array\n","        X_train = df['embedding'].to_list()\n","\n","        # Split the dataset into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(X_train, df['Generated'], test_size=0.2, random_state=42)\n","\n","        # Flatten the list of embeddings\n","        X_train = [item for sublist in X_train for item in sublist]\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        self.classifier = LogisticRegression(max_iter=1000)\n","        self.classifier.fit(X_train, y_train)\n","        self.trained = True\n","\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","\n","    TinyBERTEmbeddingModel.train = train\n","\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","\n","        # Compute embeddings for test samples\n","        X_test = df.apply(lambda row: self.sentence_embedding_tinybert(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","\n","        # Flatten the embeddings\n","        X_test = [item for sublist in X_test for item in sublist]\n","\n","        # Make predictions\n","        y_pred = self.classifier.predict(X_test)\n","\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","\n","        # Compute metrics\n","        f1 = f1_score(df['Generated'], y_pred, average='weighted')\n","        accuracy = accuracy_score(df['Generated'], y_pred)\n","\n","        print('TinyBERT')\n","        print(f'Accuracy (TinyBERT): {accuracy * 100:.2f}%')\n","        print(f'F1 Score (TinyBERT): {f1:.2f}%')\n","        print(f'Training Time (TinyBERT): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time (TinyBERT): {self.prediction_time:.2f} seconds')\n","\n","class BERTEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__('BERT')\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        self.model = BertModel.from_pretrained('bert-base-uncased')\n","\n","    def train(self, df):\n","        start_time = time.time()\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","\n","        # Split the dataset into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            list(df['embedding']), df['Generated'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        start_time = time.time()\n","        self.classifier.fit(X_train, y_train)\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test = df.apply(lambda row: self.sentence_embedding(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        y_pred = self.classifier.predict(list(X_test))\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","\n","        # Compute F1 Score\n","        f1 = f1_score(df['Generated'], y_pred, average='weighted')\n","\n","        # Evaluate the model\n","        accuracy = accuracy_score(df['Generated'], y_pred)\n","        self.accuracy_percent = accuracy * 100\n","        print(self.embedding_model)\n","        print(f'Accuracy ({self.embedding_model}): {self.accuracy_percent:.2f}%')\n","        print(f'F1 Score ({self.embedding_model}): {f1:.2f}%')\n","        print(f'Embedding Time ({self.embedding_model}): {self.embedding_time:.2f} seconds')\n","        print(f'Training Time ({self.embedding_model}): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time ({self.embedding_model}): {self.prediction_time:.2f} seconds')\n","        print()\n","\n","    def sentence_embedding(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        tokens = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n","        with torch.no_grad():\n","            outputs = self.model(**tokens)\n","        last_hidden_states = outputs.last_hidden_state\n","        sentence_embedding = torch.mean(last_hidden_states, dim=1).squeeze()\n","        return sentence_embedding.numpy()\n","\n","class ELMoEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__('ELMo')\n","        options_file = 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json'\n","        weight_file = 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'\n","        self.elmo = Elmo(options_file, weight_file, 1, dropout=0)\n","\n","    def train(self, df):\n","        start_time = time.time()\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","\n","        # Split the dataset into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            list(df['embedding']), df['Generated'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        start_time = time.time()\n","        self.classifier.fit(X_train, y_train)\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test = df.apply(lambda row: self.sentence_embedding(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        y_pred = self.classifier.predict(list(X_test))\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","\n","        # Compute F1 Score\n","        f1 = f1_score(df['Generated'], y_pred, average='weighted')\n","\n","        # Evaluate the model\n","        accuracy = accuracy_score(df['Generated'], y_pred)\n","        self.accuracy_percent = accuracy * 100\n","        print(self.embedding_model)\n","        print(f'Accuracy ({self.embedding_model}): {self.accuracy_percent:.2f}%')\n","        print(f'F1 Score ({self.embedding_model}): {f1:.2f}%')\n","        print(f'Embedding Time ({self.embedding_model}): {self.embedding_time:.2f} seconds')\n","        print(f'Training Time ({self.embedding_model}): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time ({self.embedding_model}): {self.prediction_time:.2f} seconds')\n","        print()\n","\n","    def sentence_embedding(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        character_ids = batch_to_ids([sentence.split()])  # assuming a single sentence\n","        embeddings = self.elmo(character_ids)\n","        sentence_embedding = torch.mean(embeddings['elmo_representations'][0], dim=1).squeeze()\n","        return sentence_embedding.detach().numpy()\n","\n","class USEEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__('USE')\n","        self.use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n","\n","    def train(self, df):\n","        start_time = time.time()\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","\n","        # Split the dataset into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            list(df['embedding']), df['Generated'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        start_time = time.time()\n","        self.classifier.fit(X_train, y_train)\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test = df.apply(lambda row: self.sentence_embedding(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        y_pred = self.classifier.predict(list(X_test))\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","\n","        # Compute F1 Score\n","        f1 = f1_score(df['Generated'], y_pred, average='weighted')\n","\n","        # Evaluate the model\n","        accuracy = accuracy_score(df['Generated'], y_pred)\n","        self.accuracy_percent = accuracy * 100\n","        print(self.embedding_model)\n","        print(f'Accuracy ({self.embedding_model}): {self.accuracy_percent:.2f}%')\n","        print(f'F1 Score ({self.embedding_model}): {f1:.2f}%')\n","        print(f'Embedding Time ({self.embedding_model}): {self.embedding_time:.2f} seconds')\n","        print(f'Training Time ({self.embedding_model}): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time ({self.embedding_model}): {self.prediction_time:.2f} seconds')\n","        print()\n","\n","    def sentence_embedding(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        embedding = self.use_model([sentence])[0].numpy()\n","        return embedding\n","\n","class Doc2VecEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__('Doc2Vec')\n","        self.doc2vec_model = Doc2Vec(vector_size=300, window=5, min_count=1, workers=4, epochs=20)\n","\n","    def preprocess_text(self, text):\n","        return word_tokenize(text.lower())\n","\n","    def train(self, df):\n","        # Tokenize and preprocess text for Doc2Vec\n","        df['tokenized_text'] = df.apply(lambda row: self.preprocess_text(row['Task name'] + ' ' + row['Type'] + ' ' + row['Intensity']), axis=1)\n","\n","        start_time = time.time()\n","        documents = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(df['tokenized_text'])]\n","        self.doc2vec_model.build_vocab(documents)\n","        self.doc2vec_model.train(documents, total_examples=self.doc2vec_model.corpus_count, epochs=self.doc2vec_model.epochs)\n","\n","        # Create document embeddings for the entire dataset using Doc2Vec\n","        df['embedding'] = df['tokenized_text'].apply(lambda x: self.doc2vec_model.infer_vector(x))\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","\n","        # Split the dataset into training and testing sets for Doc2Vec\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            list(df['embedding']), df['Generated'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set for Doc2Vec\n","        start_time = time.time()\n","        self.classifier.fit(X_train, y_train)\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test = df['tokenized_text'].apply(lambda x: self.doc2vec_model.infer_vector(x))\n","        y_pred = self.classifier.predict(list(X_test))\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","\n","        # Compute Accuracy\n","        accuracy = accuracy_score(df['Generated'], y_pred)\n","        self.accuracy_percent = accuracy * 100\n","\n","        # Compute F1 Score\n","        f1 = f1_score(df['Generated'], y_pred, average='weighted')\n","\n","        print(self.embedding_model)\n","        print(f'Accuracy ({self.embedding_model}): {self.accuracy_percent:.2f}%')\n","        print(f'F1 Score ({self.embedding_model}): {f1:.2f}%')\n","        print(f'Embedding Time ({self.embedding_model}): {self.embedding_time:.2f} seconds')\n","        print(f'Training Time ({self.embedding_model}): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time ({self.embedding_model}): {self.prediction_time:.2f} seconds')\n","        print()\n","\n","# # Check DataFrame columns\n","# print(df.columns)\n","\n","# # Inspect the first few rows of the DataFrame\n","# print(df.head())\n","\n","# Load your CSV dataset\n","df = pd.read_csv('/content/drive/My Drive/Sem-4/Essence/dataset-20240111.csv')\n","\n","# Instantiate models\n","bert_model = BERTEmbeddingModel()\n","elmo_model = ELMoEmbeddingModel()\n","use_model = USEEmbeddingModel()\n","doc2vec_model = Doc2VecEmbeddingModel()\n","tinybert_model = TinyBERTEmbeddingModel()\n","\n","# Train models\n","bert_model.train(df)\n","elmo_model.train(df)\n","use_model.train(df)\n","doc2vec_model.train(df)\n","tinybert_model.train(df)\n","\n","# Make predictions\n","bert_model.predict(df)\n","elmo_model.predict(df)\n","use_model.predict(df)\n","doc2vec_model.predict(df)\n","tinybert_model.predict(df)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DsfB_uH4KJ-l","executionInfo":{"status":"ok","timestamp":1712078282797,"user_tz":-330,"elapsed":985223,"user":{"displayName":"Karmishtha Patnaik","userId":"00266511564033128500"}},"outputId":"4acd2490-2290-48c8-895f-f264c3cb5dba"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["BERT\n","Accuracy (BERT): 96.93%\n","F1 Score (BERT): 0.97%\n","Embedding Time (BERT): 81.74 seconds\n","Training Time (BERT): 0.09 seconds\n","Prediction Time (BERT): 105.31 seconds\n","\n","ELMo\n","Accuracy (ELMo): 99.44%\n","F1 Score (ELMo): 0.99%\n","Embedding Time (ELMo): 305.09 seconds\n","Training Time (ELMo): 0.12 seconds\n","Prediction Time (ELMo): 298.75 seconds\n","\n","USE\n","Accuracy (USE): 87.43%\n","F1 Score (USE): 0.87%\n","Embedding Time (USE): 5.09 seconds\n","Training Time (USE): 0.02 seconds\n","Prediction Time (USE): 3.05 seconds\n","\n","Doc2Vec\n","Accuracy (Doc2Vec): 56.01%\n","F1 Score (Doc2Vec): 0.42%\n","Embedding Time (Doc2Vec): 1.33 seconds\n","Training Time (Doc2Vec): 0.01 seconds\n","Prediction Time (Doc2Vec): 0.68 seconds\n","\n","TinyBERT\n","Accuracy (TinyBERT): 96.93%\n","F1 Score (TinyBERT): 0.97%\n","Training Time (TinyBERT): 80.54 seconds\n","Prediction Time (TinyBERT): 80.11 seconds\n"]}]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HDI0SwhuOK1f","executionInfo":{"status":"ok","timestamp":1712079483692,"user_tz":-330,"elapsed":768776,"user":{"displayName":"Karmishtha Patnaik","userId":"00266511564033128500"}},"outputId":"79b0ffaf-3b54-459e-eea2-11d54ad4ec41"},"outputs":[{"output_type":"stream","name":"stdout","text":["BERT\n","Accuracy (BERT): 96.93%\n","F1 Score (BERT): 0.97%\n","Embedding Time (BERT): 83.35 seconds\n","Training Time (BERT): 0.09 seconds\n","Prediction Time (BERT): 84.13 seconds\n","\n","ELMo\n","Accuracy (ELMo): 99.44%\n","F1 Score (ELMo): 0.99%\n","Embedding Time (ELMo): 283.50 seconds\n","Training Time (ELMo): 0.10 seconds\n","Prediction Time (ELMo): 281.00 seconds\n","\n","USE\n","Accuracy (USE): 87.43%\n","F1 Score (USE): 0.87%\n","Embedding Time (USE): 4.96 seconds\n","Training Time (USE): 0.02 seconds\n","Prediction Time (USE): 3.29 seconds\n","\n","Doc2Vec\n","Accuracy (Doc2Vec): 56.01%\n","F1 Score (Doc2Vec): 0.42%\n","Embedding Time (Doc2Vec): 1.35 seconds\n","Training Time (Doc2Vec): 0.01 seconds\n","Prediction Time (Doc2Vec): 0.69 seconds\n","\n"]}],"source":["import time\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, f1_score\n","import torch\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from nltk.tokenize import word_tokenize\n","from transformers import BertTokenizer, BertModel\n","from allennlp.modules.elmo import Elmo, batch_to_ids\n","\n","class EmbeddingModel:\n","    def __init__(self, embedding_model):\n","        self.trained = False\n","        self.classifier= LogisticRegression(max_iter=1000)\n","        self.embedding_model = embedding_model\n","\n","    def train(self, df):\n","        raise NotImplementedError\n","\n","    def predict(self, df):\n","        raise NotImplementedError\n","\n","    def sentence_embedding(self, task_name, task_type, task_intensity):\n","        raise NotImplementedError\n","\n","class BERTEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__('BERT')\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        self.model = BertModel.from_pretrained('bert-base-uncased')\n","\n","    def train(self, df):\n","        start_time = time.time()\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","\n","        # Split the dataset into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            list(df['embedding']), df['Generated'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        start_time = time.time()\n","        self.classifier.fit(X_train, y_train)\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test = df.apply(lambda row: self.sentence_embedding(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        y_pred = self.classifier.predict(list(X_test))\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","\n","        # Compute F1 Score\n","        f1 = f1_score(df['Generated'], y_pred, average='weighted')\n","\n","        # Evaluate the model\n","        accuracy = accuracy_score(df['Generated'], y_pred)\n","        self.accuracy_percent = accuracy * 100\n","        print(self.embedding_model)\n","        print(f'Accuracy ({self.embedding_model}): {self.accuracy_percent:.2f}%')\n","        print(f'F1 Score ({self.embedding_model}): {f1:.2f}%')\n","        print(f'Embedding Time ({self.embedding_model}): {self.embedding_time:.2f} seconds')\n","        print(f'Training Time ({self.embedding_model}): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time ({self.embedding_model}): {self.prediction_time:.2f} seconds')\n","        print()\n","\n","    def sentence_embedding(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        tokens = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n","        with torch.no_grad():\n","            outputs = self.model(**tokens)\n","        last_hidden_states = outputs.last_hidden_state\n","        sentence_embedding = torch.mean(last_hidden_states, dim=1).squeeze()\n","        return sentence_embedding.numpy()\n","\n","class ELMoEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__('ELMo')\n","        options_file = 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json'\n","        weight_file = 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'\n","        self.elmo = Elmo(options_file, weight_file, 1, dropout=0)\n","\n","    def train(self, df):\n","        start_time = time.time()\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","\n","        # Split the dataset into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            list(df['embedding']), df['Generated'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        start_time = time.time()\n","        self.classifier.fit(X_train, y_train)\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test = df.apply(lambda row: self.sentence_embedding(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        y_pred = self.classifier.predict(list(X_test))\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","\n","        # Compute F1 Score\n","        f1 = f1_score(df['Generated'], y_pred, average='weighted')\n","\n","        # Evaluate the model\n","        accuracy = accuracy_score(df['Generated'], y_pred)\n","        self.accuracy_percent = accuracy * 100\n","        print(self.embedding_model)\n","        print(f'Accuracy ({self.embedding_model}): {self.accuracy_percent:.2f}%')\n","        print(f'F1 Score ({self.embedding_model}): {f1:.2f}%')\n","        print(f'Embedding Time ({self.embedding_model}): {self.embedding_time:.2f} seconds')\n","        print(f'Training Time ({self.embedding_model}): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time ({self.embedding_model}): {self.prediction_time:.2f} seconds')\n","        print()\n","\n","    def sentence_embedding(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        character_ids = batch_to_ids([sentence.split()])  # assuming a single sentence\n","        embeddings = self.elmo(character_ids)\n","        sentence_embedding = torch.mean(embeddings['elmo_representations'][0], dim=1).squeeze()\n","        return sentence_embedding.detach().numpy()\n","\n","class USEEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__('USE')\n","        self.use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n","\n","    def train(self, df):\n","        start_time = time.time()\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","\n","        # Split the dataset into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            list(df['embedding']), df['Generated'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        start_time = time.time()\n","        self.classifier.fit(X_train, y_train)\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test = df.apply(lambda row: self.sentence_embedding(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        y_pred = self.classifier.predict(list(X_test))\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","\n","        # Compute F1 Score\n","        f1 = f1_score(df['Generated'], y_pred, average='weighted')\n","\n","        # Evaluate the model\n","        accuracy = accuracy_score(df['Generated'], y_pred)\n","        self.accuracy_percent = accuracy * 100\n","        print(self.embedding_model)\n","        print(f'Accuracy ({self.embedding_model}): {self.accuracy_percent:.2f}%')\n","        print(f'F1 Score ({self.embedding_model}): {f1:.2f}%')\n","        print(f'Embedding Time ({self.embedding_model}): {self.embedding_time:.2f} seconds')\n","        print(f'Training Time ({self.embedding_model}): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time ({self.embedding_model}): {self.prediction_time:.2f} seconds')\n","        print()\n","\n","    def sentence_embedding(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        embedding = self.use_model([sentence])[0].numpy()\n","        return embedding\n","\n","class Doc2VecEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__('Doc2Vec')\n","        self.doc2vec_model = Doc2Vec(vector_size=300, window=5, min_count=1, workers=4, epochs=20)\n","\n","    def preprocess_text(self, text):\n","        return word_tokenize(text.lower())\n","\n","    def train(self, df):\n","        # Tokenize and preprocess text for Doc2Vec\n","        df['tokenized_text'] = df.apply(lambda row: self.preprocess_text(row['Task name'] + ' ' + row['Type'] + ' ' + row['Intensity']), axis=1)\n","\n","        start_time = time.time()\n","        documents = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(df['tokenized_text'])]\n","        self.doc2vec_model.build_vocab(documents)\n","        self.doc2vec_model.train(documents, total_examples=self.doc2vec_model.corpus_count, epochs=self.doc2vec_model.epochs)\n","\n","        # Create document embeddings for the entire dataset using Doc2Vec\n","        df['embedding'] = df['tokenized_text'].apply(lambda x: self.doc2vec_model.infer_vector(x))\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","\n","        # Split the dataset into training and testing sets for Doc2Vec\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            list(df['embedding']), df['Generated'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set for Doc2Vec\n","        start_time = time.time()\n","        self.classifier.fit(X_train, y_train)\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test = df['tokenized_text'].apply(lambda x: self.doc2vec_model.infer_vector(x))\n","        y_pred = self.classifier.predict(list(X_test))\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","\n","        # Compute Accuracy\n","        accuracy = accuracy_score(df['Generated'], y_pred)\n","        self.accuracy_percent = accuracy * 100\n","\n","        # Compute F1 Score\n","        f1 = f1_score(df['Generated'], y_pred, average='weighted')\n","\n","        print(self.embedding_model)\n","        print(f'Accuracy ({self.embedding_model}): {self.accuracy_percent:.2f}%')\n","        print(f'F1 Score ({self.embedding_model}): {f1:.2f}%')\n","        print(f'Embedding Time ({self.embedding_model}): {self.embedding_time:.2f} seconds')\n","        print(f'Training Time ({self.embedding_model}): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time ({self.embedding_model}): {self.prediction_time:.2f} seconds')\n","        print()\n","\n","# # Check DataFrame columns\n","# print(df.columns)\n","\n","# # Inspect the first few rows of the DataFrame\n","# print(df.head())\n","\n","# Load your CSV dataset\n","df = pd.read_csv('/content/drive/My Drive/Sem-4/Essence/dataset-20240111.csv')\n","\n","# Instantiate models\n","bert_model = BERTEmbeddingModel()\n","elmo_model = ELMoEmbeddingModel()\n","use_model = USEEmbeddingModel()\n","doc2vec_model = Doc2VecEmbeddingModel()\n","\n","# Train models\n","bert_model.train(df)\n","elmo_model.train(df)\n","use_model.train(df)\n","doc2vec_model.train(df)\n","\n","# Make predictions\n","bert_model.predict(df)\n","elmo_model.predict(df)\n","use_model.predict(df)\n","doc2vec_model.predict(df)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jOPiDmpa3KJ-"},"source":["#All the codes with Tiny bert"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":985127,"status":"ok","timestamp":1710326956239,"user":{"displayName":"Karmishtha Patnaik","userId":"00266511564033128500"},"user_tz":-330},"id":"yrt1VPMBCKBV","outputId":"49d096ef-3c74-4f5e-88e8-287d2ad87671"},"outputs":[{"name":"stdout","output_type":"stream","text":["TinyBERT\n","Accuracy (TinyBERT): 94.55%\n","F1 Score (TinyBERT): 0.95%\n","Training Time (TinyBERT): 76.15 seconds\n","Embedding Time (TinyBERT): 0.05 seconds\n","Prediction Time (TinyBERT): 47.81 seconds\n","BERT\n","Accuracy (BERT): 96.93%\n","F1 Score (BERT): 0.97%\n","Embedding Time (BERT): 79.56 seconds\n","Training Time (BERT): 0.21 seconds\n","Prediction Time (BERT): 76.20 seconds\n","\n","ELMO\n","Accuracy (ELMo): 99.44%\n","F1 Score (BERT): 0.99%\n","Embedding Time (ELMo): 285.75 seconds\n","Training Time (ELMo): 0.10 seconds\n","Prediction Time (ELMo): 382.75 seconds\n","\n","USE\n","Accuracy (USE): 87.43%\n","F1 Score: 0.87%\n","Embedding Time (USE): 3.50 seconds\n","Training Time (USE): 0.02 seconds\n","Prediction Time (USE): 3.57 seconds\n","\n","Doc2vec\n","Accuracy (Doc2Vec): 56.01%\n","F1 Score (Doc2Vec): 0.42%\n","Embedding Time (Doc2Vec): 1.35 seconds\n","Training Time (Doc2Vec): 0.01 seconds\n","Prediction Time (Doc2Vec): 0.76 seconds\n","\n"]}],"source":["import time\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","import torch\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from nltk.tokenize import word_tokenize\n","from transformers import BertTokenizer, BertModel\n","from allennlp.modules.elmo import Elmo, batch_to_ids\n","\n","class EmbeddingModel:\n","    def __init__(self):\n","        self.trained = False\n","\n","    def train(self, df):\n","        pass\n","\n","    def predict(self, df):\n","        pass\n","\n","class BERTEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__()\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        self.model = BertModel.from_pretrained('bert-base-uncased')\n","        self.classifier = LogisticRegression(max_iter=1000)\n","\n","    def train(self, df):\n","        start_time = time.time()\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding_bert(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","\n","        # Split the dataset into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            list(df['embedding']), df['ChatGPT'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        start_time = time.time()\n","        self.classifier.fit(X_train, y_train)\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test = df.apply(lambda row: self.sentence_embedding_bert(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        y_pred = self.classifier.predict(list(X_test))\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","        # Compute F1 Score\n","        f1 = f1_score(df['ChatGPT'], y_pred, average='weighted')\n","\n","\n","        # Evaluate the model\n","        accuracy = accuracy_score(df['ChatGPT'], y_pred)\n","        self.accuracy_percent = accuracy * 100\n","        print('BERT')\n","        print(f'Accuracy (BERT): {self.accuracy_percent:.2f}%')\n","        print(f'F1 Score (BERT): {f1:.2f}%')\n","        print(f'Embedding Time (BERT): {self.embedding_time:.2f} seconds')\n","        print(f'Training Time (BERT): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time (BERT): {self.prediction_time:.2f} seconds')\n","        print()\n","\n","    def sentence_embedding_bert(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        tokens = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n","        with torch.no_grad():\n","            outputs = self.model(**tokens)\n","        last_hidden_states = outputs.last_hidden_state\n","        sentence_embedding = torch.mean(last_hidden_states, dim=1).squeeze()\n","        return sentence_embedding.numpy()\n","\n","class ELMoEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__()\n","        options_file = 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json'\n","        weight_file = 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'\n","        self.elmo = Elmo(options_file, weight_file, 1, dropout=0)\n","        self.classifier = LogisticRegression(max_iter=1000)  # Add this line\n","\n","    def train(self, df):\n","        start_time = time.time()\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding_elmo(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","\n","        # Split the dataset into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            list(df['embedding']), df['ChatGPT'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        start_time = time.time()\n","        self.classifier.fit(X_train, y_train)\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test = df.apply(lambda row: self.sentence_embedding_elmo(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        y_pred = self.classifier.predict(list(X_test))\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","\n","        # Compute F1 Score\n","        f1 = f1_score(df['ChatGPT'], y_pred, average='weighted')\n","\n","\n","        # Evaluate the model\n","        accuracy = accuracy_score(df['ChatGPT'], y_pred)\n","        self.accuracy_percent = accuracy * 100\n","\n","        print('ELMO')\n","        print(f'Accuracy (ELMo): {self.accuracy_percent:.2f}%')\n","        print(f'F1 Score (BERT): {f1:.2f}%')\n","        print(f'Embedding Time (ELMo): {self.embedding_time:.2f} seconds')\n","        print(f'Training Time (ELMo): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time (ELMo): {self.prediction_time:.2f} seconds')\n","        print()\n","\n","    def sentence_embedding_elmo(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        character_ids = batch_to_ids([sentence.split()])  # assuming a single sentence\n","        embeddings = self.elmo(character_ids)\n","        sentence_embedding = torch.mean(embeddings['elmo_representations'][0], dim=1).squeeze()\n","        return sentence_embedding.detach().numpy()\n","\n","class USEEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__()\n","        self.use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n","        self.classifier = LogisticRegression(max_iter=1000)\n","\n","    def train(self, df):\n","        start_time = time.time()\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding_use(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","\n","        # Split the dataset into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            list(df['embedding']), df['ChatGPT'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        start_time = time.time()\n","        self.classifier.fit(X_train, y_train)\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test = df.apply(lambda row: self.sentence_embedding_use(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        y_pred = self.classifier.predict(list(X_test))\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","         # Compute F1 Score\n","        f1 = f1_score(df['ChatGPT'], y_pred, average='weighted')\n","\n","\n","        # Evaluate the model\n","        accuracy = accuracy_score(df['ChatGPT'], y_pred)\n","        self.accuracy_percent = accuracy * 100\n","        print('USE')\n","        print(f'Accuracy (USE): {self.accuracy_percent:.2f}%')\n","        # Compute F1 Score\n","        f1 = f1_score(df['ChatGPT'], y_pred, average='weighted')\n","        print(f'F1 Score: {f1:.2f}%')\n","        print(f'Embedding Time (USE): {self.embedding_time:.2f} seconds')\n","        print(f'Training Time (USE): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time (USE): {self.prediction_time:.2f} seconds')\n","        print( )\n","    def sentence_embedding_use(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        embedding = self.use_model([sentence])[0].numpy()\n","        return embedding\n","\n","class EmbeddingModel:\n","    def __init__(self):\n","        self.trained = False\n","\n","    def train(self, df):\n","        pass\n","\n","    def predict(self, df):\n","        pass\n","\n","class Doc2VecEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__()\n","        self.doc2vec_model = Doc2Vec(vector_size=300, window=5, min_count=1, workers=4, epochs=20)\n","        self.classifier = LogisticRegression(max_iter=1000)\n","        self.embedding_time_doc2vec = 0\n","        self.training_time_doc2vec = 0\n","        self.prediction_time_doc2vec = 0\n","        self.accuracy_percent_doc2vec = 0\n","\n","    def preprocess_text(self, text):\n","        return word_tokenize(text.lower())\n","\n","    def train(self, df):\n","        # Tokenize and preprocess text for Doc2Vec\n","        df['tokenized_text'] = df.apply(lambda row: self.preprocess_text(row['Task name'] + ' ' + row['Type'] + ' ' + row['Intensity']), axis=1)\n","\n","        start_time = time.time()\n","        documents = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(df['tokenized_text'])]\n","        self.doc2vec_model.build_vocab(documents)\n","        self.doc2vec_model.train(documents, total_examples=self.doc2vec_model.corpus_count, epochs=self.doc2vec_model.epochs)\n","\n","        # Create document embeddings for the entire dataset using Doc2Vec\n","        df['embedding_doc2vec'] = df['tokenized_text'].apply(lambda x: self.doc2vec_model.infer_vector(x))\n","        end_time = time.time()\n","        self.embedding_time_doc2vec = end_time - start_time\n","\n","        # Split the dataset into training and testing sets for Doc2Vec\n","        X_train_doc2vec, X_test_doc2vec, y_train_doc2vec, y_test_doc2vec = train_test_split(\n","            list(df['embedding_doc2vec']), df['ChatGPT'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set for Doc2Vec\n","        start_time = time.time()\n","        self.classifier.fit(X_train_doc2vec, y_train_doc2vec)\n","        end_time = time.time()\n","        self.training_time_doc2vec = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test_doc2vec = df['tokenized_text'].apply(lambda x: self.doc2vec_model.infer_vector(x))\n","        y_pred_doc2vec = self.classifier.predict(list(X_test_doc2vec))\n","        end_time = time.time()\n","        self.prediction_time_doc2vec = end_time - start_time\n","\n","        # Compute Accuracy\n","        accuracy_doc2vec = accuracy_score(df['ChatGPT'], y_pred_doc2vec)\n","        self.accuracy_percent_doc2vec = accuracy_doc2vec * 100\n","\n","        # Compute F1 Score\n","        f1 = f1_score(df['ChatGPT'], y_pred_doc2vec, average='weighted')\n","\n","        print('Doc2vec')\n","        print(f'Accuracy (Doc2Vec): {self.accuracy_percent_doc2vec:.2f}%')\n","        print(f'F1 Score (Doc2Vec): {f1:.2f}%')\n","        print(f'Embedding Time (Doc2Vec): {self.embedding_time_doc2vec:.2f} seconds')\n","        print(f'Training Time (Doc2Vec): {self.training_time_doc2vec:.2f} seconds')\n","        print(f'Prediction Time (Doc2Vec): {self.prediction_time_doc2vec:.2f} seconds')\n","        print()\n","\n","class TinyBERTEmbeddingModel:\n","    def __init__(self):\n","        self.model = SentenceTransformer('paraphrase-TinyBERT-L6-v2')\n","        self.trained = False\n","\n","    def train(self, df):\n","        start_time = time.time()\n","\n","        # Compute embeddings for all samples\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding_tinybert(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","\n","        # Flatten the embeddings\n","        X_train = df['embedding'].to_list()\n","\n","        # Flatten the list of embeddings\n","        X_train = [item for sublist in X_train for item in sublist]\n","\n","        # Split the dataset into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(X_train, df['ChatGPT'], test_size=0.2, random_state=42)\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        self.classifier = LogisticRegression(max_iter=1000)\n","        self.classifier.fit(X_train, y_train)\n","        self.trained = True\n","\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","\n","        # Compute embeddings for test samples\n","        X_test = df.apply(lambda row: self.sentence_embedding_tinybert(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","\n","        # Flatten the embeddings\n","        X_test = [item for sublist in X_test for item in sublist]\n","\n","        # Make predictions\n","        y_pred = self.classifier.predict(X_test)\n","\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","\n","        # Compute metrics\n","        f1 = f1_score(df['ChatGPT'], y_pred, average='weighted')\n","        accuracy = accuracy_score(df['ChatGPT'], y_pred)\n","\n","        print('TinyBERT')\n","        print(f'Accuracy (TinyBERT): {accuracy * 100:.2f}%')\n","        print(f'F1 Score (TinyBERT): {f1:.2f}%')\n","        print(f'Training Time (TinyBERT): {self.training_time:.2f} seconds')\n","        print(f'Embedding Time (TinyBERT): {self.embedding_time:.2f} seconds')\n","        print(f'Prediction Time (TinyBERT): {self.prediction_time:.2f} seconds')\n","\n","    def sentence_embedding_tinybert(self, task_name, task_type, task_intensity):\n","        start_time = time.time()\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        embedding = self.model.encode([sentence])\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","        return embedding\n","\n","# Load your CSV dataset\n","df = pd.read_csv('/content/drive/My Drive/Sem-4/Essence/dataset-20240111.csv')\n","\n","# Load your CSV dataset\n","df = pd.read_csv('/content/drive/My Drive/Sem-4/Essence/dataset-20240111.csv')\n","\n","# Instantiate models\n","bert_model = BERTEmbeddingModel()\n","elmo_model = ELMoEmbeddingModel()\n","use_model = USEEmbeddingModel()\n","doc2vec_model = Doc2VecEmbeddingModel()\n","tinybert_model = TinyBERTEmbeddingModel()\n","\n","\n","# Train models\n","bert_model.train(df)\n","elmo_model.train(df)\n","use_model.train(df)\n","doc2vec_model.train(df)\n","tinybert_model.train(df)\n","\n","\n","# Make predictions\n","bert_model.predict(df)\n","elmo_model.predict(df)\n","use_model.predict(df)\n","doc2vec_model.predict(df)\n","tinybert_model.predict(df)\n"]},{"cell_type":"markdown","metadata":{"id":"hjUC7UfRBrw-"},"source":["# 1)With F1 Score and Encapsulation"]},{"cell_type":"markdown","metadata":{"id":"jJCwq5ZHBzX5"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":728,"referenced_widgets":["74309ede0fbc4c27afc26f2d0fc84707","26eede88f67f43fa8eb9d557bcc1164b","1b256886670d4591a731a4f5cb0b786e","d13afc2439074a919103f3f8f790e9e1","95f9678ab62f4166b049671158439b1b","e2931072c6be4626b472b9fc503bf228","77bd4f6dd4004b1b9e16e9fe8a845c52","6b741ffd2f6a403f997a8a96b8ad2708","5c09c69a6a514cb48ab4f6e21478e72f","cb066456d7384fa7a0048633b9af9d45","d4a63a52f517429ca505dba0e3c2c8fb","138fcbcf0c8a488490c357e66eca7c46","87a726e6a6524e57b40e65e524715389","f3ed01ce39034017a318a9c87138dc0e","748d499d3538475d9c9f8c19e0d370ea","d54514eebfc24750bb317db82300a3fb","e7ee32ed2eaa443b9bc86185bdc2ae0e","36edc281b9554008b98f9097698061a1","bf39865316fe42978e6a19745f082a2a","c324b874ffd74ed69dcbac524e5dd484","a95b323589214d87be6c5c672b4c6104","8f9fa2000111488bb072daa77dd2b29a","63b041de84e243c1a505c567c8c22a80","a7b3db7e4e5a40ca852759e3b1124f5e","06093853f44f4b018ac428749519aa5a","3563cb17af1e4763b80beb3312a67fe7","935f33b7996d468e93f35a4685ffc67c","6f1042cfd1124157bd73dfd1be66d99d","920c8755cef9445fa98336ac7bdafc78","60cdf0c1e4e8464ea70fbffeda452b6f","9e6a08eb48ed4eabb8e188f5f680d581","203d333f5f2f40b48748590e505c0d4f","934128c214ed41ca8095beb4251b4c5a","b1b65d7bcd204ac9b1d682b90c543103","05d7d6236a854f52abd92413616f67da","6bf5464ab7284464a09c315376ebb799","c2288e65f4e0420ebfd1d3229c9609c2","5b28750eb55242f7bde2e6bb4775d374","58689c648b3e4f579560504ecf34cb23","57eaf7d0b8914adaa79c86939bc627e4","595026cc9ecc4f88bec16fab47d8e631","5796444343bd4269865a7a009b9d697c","f7c4c50cc5e24270962478bb06e01d9a","7d24c4910da54c14a5d06bde7bff1633","cdbd3ed698fc4b8dbfa675731bb1a0d7","14f2e0d4d76346ce91ee65c0a4c16340","ca62a0e7b3c744c5b1b9383a79cd109c","910a1538c5b64878909664d99dae947f","48884da94e644179bf47570d3899dedf","2158db8eff8d4ba9888c22bf2972b5a3","d430c842c9cc4bc29cf7cdb1799fe2aa","d679df0471604c2cb1e49b846a77c3fd","e702339baf01436e8d1c82a39d4b33c2","84429f2c87d342d59c37443a8fef535f","49fb6e2aed004cb599f01f517b8eb1ed","064a0f95211548abb67de2008231c18a","d07442a225e34031aa02637f03d69623","2c14bf77ce6f4d40a1b5aa607986d824","445adc0b827844d293df108e8c9935b8"]},"executionInfo":{"elapsed":847501,"status":"ok","timestamp":1710325676468,"user":{"displayName":"Karmishtha Patnaik","userId":"00266511564033128500"},"user_tz":-330},"id":"b_6UTPkVFqxe","outputId":"aee54ef6-eb93-4e01-db25-1f71af8f7f4a"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"74309ede0fbc4c27afc26f2d0fc84707","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"138fcbcf0c8a488490c357e66eca7c46","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"63b041de84e243c1a505c567c8c22a80","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1b65d7bcd204ac9b1d682b90c543103","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cdbd3ed698fc4b8dbfa675731bb1a0d7","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"064a0f95211548abb67de2008231c18a","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"],"text/plain":[]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n","</pre>\n"],"text/plain":["\n"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c14bf77ce6f4d40a1b5aa607986d824","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"],"text/plain":[]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n","</pre>\n"],"text/plain":["\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["BERT\n","Accuracy (BERT): 96.93%\n","F1 Score (BERT): 0.97%\n","Embedding Time (BERT): 81.56 seconds\n","Training Time (BERT): 0.13 seconds\n","Prediction Time (BERT): 81.47 seconds\n","\n","ELMO\n","Accuracy (ELMo): 99.44%\n","F1 Score (BERT): 0.99%\n","Embedding Time (ELMo): 314.04 seconds\n","Training Time (ELMo): 0.21 seconds\n","Prediction Time (ELMo): 301.34 seconds\n","\n","USE\n","Accuracy (USE): 87.43%\n","F1 Score: 0.87%\n","Embedding Time (USE): 4.55 seconds\n","Training Time (USE): 0.03 seconds\n","Prediction Time (USE): 3.01 seconds\n","\n","Doc2vec\n","Accuracy (Doc2Vec): 56.01%\n","F1 Score (Doc2Vec): 0.42%\n","Embedding Time (Doc2Vec): 1.41 seconds\n","Training Time (Doc2Vec): 0.01 seconds\n","Prediction Time (Doc2Vec): 0.69 seconds\n","\n"]}],"source":["import time\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","import torch\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from nltk.tokenize import word_tokenize\n","from transformers import BertTokenizer, BertModel\n","from allennlp.modules.elmo import Elmo, batch_to_ids\n","from sklearn.metrics import f1_score\n","\n","class EmbeddingModel:\n","    def __init__(self):\n","        self.trained = False\n","\n","    def train(self, df):\n","        pass\n","\n","    def predict(self, df):\n","        pass\n","\n","class BERTEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__()\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        self.model = BertModel.from_pretrained('bert-base-uncased')\n","        self.classifier = LogisticRegression(max_iter=1000)\n","\n","    def train(self, df):\n","        start_time = time.time()\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding_bert(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","\n","        # Split the dataset into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            list(df['embedding']), df['ChatGPT'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        start_time = time.time()\n","        self.classifier.fit(X_train, y_train)\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test = df.apply(lambda row: self.sentence_embedding_bert(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        y_pred = self.classifier.predict(list(X_test))\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","        # Compute F1 Score\n","        f1 = f1_score(df['ChatGPT'], y_pred, average='weighted')\n","\n","\n","        # Evaluate the model\n","        accuracy = accuracy_score(df['ChatGPT'], y_pred)\n","        self.accuracy_percent = accuracy * 100\n","        print('BERT')\n","        print(f'Accuracy (BERT): {self.accuracy_percent:.2f}%')\n","        print(f'F1 Score (BERT): {f1:.2f}%')\n","        print(f'Embedding Time (BERT): {self.embedding_time:.2f} seconds')\n","        print(f'Training Time (BERT): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time (BERT): {self.prediction_time:.2f} seconds')\n","        print()\n","\n","    def sentence_embedding_bert(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        tokens = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n","        with torch.no_grad():\n","            outputs = self.model(**tokens)\n","        last_hidden_states = outputs.last_hidden_state\n","        sentence_embedding = torch.mean(last_hidden_states, dim=1).squeeze()\n","        return sentence_embedding.numpy()\n","\n","class ELMoEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__()\n","        options_file = 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json'\n","        weight_file = 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'\n","        self.elmo = Elmo(options_file, weight_file, 1, dropout=0)\n","        self.classifier = LogisticRegression(max_iter=1000)  # Add this line\n","\n","    def train(self, df):\n","        start_time = time.time()\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding_elmo(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","\n","        # Split the dataset into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            list(df['embedding']), df['ChatGPT'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        start_time = time.time()\n","        self.classifier.fit(X_train, y_train)\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test = df.apply(lambda row: self.sentence_embedding_elmo(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        y_pred = self.classifier.predict(list(X_test))\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","\n","        # Compute F1 Score\n","        f1 = f1_score(df['ChatGPT'], y_pred, average='weighted')\n","\n","\n","        # Evaluate the model\n","        accuracy = accuracy_score(df['ChatGPT'], y_pred)\n","        self.accuracy_percent = accuracy * 100\n","\n","        print('ELMO')\n","        print(f'Accuracy (ELMo): {self.accuracy_percent:.2f}%')\n","        print(f'F1 Score (BERT): {f1:.2f}%')\n","        print(f'Embedding Time (ELMo): {self.embedding_time:.2f} seconds')\n","        print(f'Training Time (ELMo): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time (ELMo): {self.prediction_time:.2f} seconds')\n","        print()\n","\n","    def sentence_embedding_elmo(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        character_ids = batch_to_ids([sentence.split()])  # assuming a single sentence\n","        embeddings = self.elmo(character_ids)\n","        sentence_embedding = torch.mean(embeddings['elmo_representations'][0], dim=1).squeeze()\n","        return sentence_embedding.detach().numpy()\n","\n","class USEEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__()\n","        self.use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n","        self.classifier = LogisticRegression(max_iter=1000)\n","\n","    def train(self, df):\n","        start_time = time.time()\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding_use(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","\n","        # Split the dataset into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            list(df['embedding']), df['ChatGPT'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        start_time = time.time()\n","        self.classifier.fit(X_train, y_train)\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test = df.apply(lambda row: self.sentence_embedding_use(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        y_pred = self.classifier.predict(list(X_test))\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","         # Compute F1 Score\n","        f1 = f1_score(df['ChatGPT'], y_pred, average='weighted')\n","\n","\n","        # Evaluate the model\n","        accuracy = accuracy_score(df['ChatGPT'], y_pred)\n","        self.accuracy_percent = accuracy * 100\n","        print('USE')\n","        print(f'Accuracy (USE): {self.accuracy_percent:.2f}%')\n","        # Compute F1 Score\n","        f1 = f1_score(df['ChatGPT'], y_pred, average='weighted')\n","        print(f'F1 Score: {f1:.2f}%')\n","        print(f'Embedding Time (USE): {self.embedding_time:.2f} seconds')\n","        print(f'Training Time (USE): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time (USE): {self.prediction_time:.2f} seconds')\n","        print( )\n","    def sentence_embedding_use(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        embedding = self.use_model([sentence])[0].numpy()\n","        return embedding\n","\n","class EmbeddingModel:\n","    def __init__(self):\n","        self.trained = False\n","\n","    def train(self, df):\n","        pass\n","\n","    def predict(self, df):\n","        pass\n","\n","class Doc2VecEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__()\n","        self.doc2vec_model = Doc2Vec(vector_size=300, window=5, min_count=1, workers=4, epochs=20)\n","        self.classifier = LogisticRegression(max_iter=1000)\n","        self.embedding_time_doc2vec = 0\n","        self.training_time_doc2vec = 0\n","        self.prediction_time_doc2vec = 0\n","        self.accuracy_percent_doc2vec = 0\n","\n","    def preprocess_text(self, text):\n","        return word_tokenize(text.lower())\n","\n","    def train(self, df):\n","        # Tokenize and preprocess text for Doc2Vec\n","        df['tokenized_text'] = df.apply(lambda row: self.preprocess_text(row['Task name'] + ' ' + row['Type'] + ' ' + row['Intensity']), axis=1)\n","\n","        start_time = time.time()\n","        documents = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(df['tokenized_text'])]\n","        self.doc2vec_model.build_vocab(documents)\n","        self.doc2vec_model.train(documents, total_examples=self.doc2vec_model.corpus_count, epochs=self.doc2vec_model.epochs)\n","\n","        # Create document embeddings for the entire dataset using Doc2Vec\n","        df['embedding_doc2vec'] = df['tokenized_text'].apply(lambda x: self.doc2vec_model.infer_vector(x))\n","        end_time = time.time()\n","        self.embedding_time_doc2vec = end_time - start_time\n","\n","        # Split the dataset into training and testing sets for Doc2Vec\n","        X_train_doc2vec, X_test_doc2vec, y_train_doc2vec, y_test_doc2vec = train_test_split(\n","            list(df['embedding_doc2vec']), df['ChatGPT'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set for Doc2Vec\n","        start_time = time.time()\n","        self.classifier.fit(X_train_doc2vec, y_train_doc2vec)\n","        end_time = time.time()\n","        self.training_time_doc2vec = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test_doc2vec = df['tokenized_text'].apply(lambda x: self.doc2vec_model.infer_vector(x))\n","        y_pred_doc2vec = self.classifier.predict(list(X_test_doc2vec))\n","        end_time = time.time()\n","        self.prediction_time_doc2vec = end_time - start_time\n","\n","        # Compute Accuracy\n","        accuracy_doc2vec = accuracy_score(df['ChatGPT'], y_pred_doc2vec)\n","        self.accuracy_percent_doc2vec = accuracy_doc2vec * 100\n","\n","        # Compute F1 Score\n","        f1 = f1_score(df['ChatGPT'], y_pred_doc2vec, average='weighted')\n","\n","        print('Doc2vec')\n","        print(f'Accuracy (Doc2Vec): {self.accuracy_percent_doc2vec:.2f}%')\n","        print(f'F1 Score (Doc2Vec): {f1:.2f}%')\n","        print(f'Embedding Time (Doc2Vec): {self.embedding_time_doc2vec:.2f} seconds')\n","        print(f'Training Time (Doc2Vec): {self.training_time_doc2vec:.2f} seconds')\n","        print(f'Prediction Time (Doc2Vec): {self.prediction_time_doc2vec:.2f} seconds')\n","        print()\n","# Load your CSV dataset\n","df = pd.read_csv('/content/drive/My Drive/Sem-4/Essence/dataset-20240111.csv')\n","\n","# Instantiate models\n","bert_model = BERTEmbeddingModel()\n","elmo_model = ELMoEmbeddingModel()\n","use_model = USEEmbeddingModel()\n","doc2vec_model = Doc2VecEmbeddingModel()\n","\n","# Train models\n","bert_model.train(df)\n","elmo_model.train(df)\n","use_model.train(df)\n","doc2vec_model.train(df)\n","\n","# Make predictions\n","bert_model.predict(df)\n","elmo_model.predict(df)\n","use_model.predict(df)\n","doc2vec_model.predict(df)\n"]},{"cell_type":"markdown","metadata":{"id":"NXybvpORCAoO"},"source":["#2)With Encapsulation but Without F1-Score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":703565,"status":"ok","timestamp":1709532837368,"user":{"displayName":"Karmishtha Patnaik","userId":"00266511564033128500"},"user_tz":-330},"id":"1YhiJNl55-NB","outputId":"ad2e26ea-c610-4ae5-f871-0c177ccf21cf"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["BERT\n","Accuracy (BERT): 96.93%\n","Embedding Time (BERT): 69.85 seconds\n","Training Time (BERT): 0.10 seconds\n","Prediction Time (BERT): 69.61 seconds\n","\n","ELMO\n","Accuracy (ELMo): 99.44%\n","Embedding Time (ELMo): 260.91 seconds\n","Training Time (ELMo): 0.09 seconds\n","Prediction Time (ELMo): 271.11 seconds\n","\n","USE\n","Accuracy (USE): 87.43%\n","Embedding Time (USE): 3.22 seconds\n","Training Time (USE): 0.02 seconds\n","Prediction Time (USE): 4.31 seconds\n","\n","Doc2vec\n","Accuracy (Doc2Vec): 56.01%\n","Embedding Time (Doc2Vec): 1.20 seconds\n","Training Time (Doc2Vec): 0.01 seconds\n","Prediction Time (Doc2Vec): 0.63 seconds\n","\n"]}],"source":["import time\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","import torch\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from nltk.tokenize import word_tokenize\n","from transformers import BertTokenizer, BertModel\n","from allennlp.modules.elmo import Elmo, batch_to_ids\n","\n","\n","class EmbeddingModel:\n","    def __init__(self):\n","        self.trained = False\n","\n","    def train(self, df):\n","        pass\n","\n","    def predict(self, df):\n","        pass\n","\n","class BERTEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__()\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        self.model = BertModel.from_pretrained('bert-base-uncased')\n","        self.classifier = LogisticRegression(max_iter=1000)\n","\n","    def train(self, df):\n","        start_time = time.time()\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding_bert(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","\n","        # Split the dataset into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            list(df['embedding']), df['ChatGPT'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        start_time = time.time()\n","        self.classifier.fit(X_train, y_train)\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test = df.apply(lambda row: self.sentence_embedding_bert(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        y_pred = self.classifier.predict(list(X_test))\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","\n","\n","\n","        # Evaluate the model\n","        accuracy = accuracy_score(df['ChatGPT'], y_pred)\n","        self.accuracy_percent = accuracy * 100\n","        print('BERT')\n","        print(f'Accuracy (BERT): {self.accuracy_percent:.2f}%')\n","\n","        print(f'Embedding Time (BERT): {self.embedding_time:.2f} seconds')\n","        print(f'Training Time (BERT): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time (BERT): {self.prediction_time:.2f} seconds')\n","        print()\n","\n","    def sentence_embedding_bert(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        tokens = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n","        with torch.no_grad():\n","            outputs = self.model(**tokens)\n","        last_hidden_states = outputs.last_hidden_state\n","        sentence_embedding = torch.mean(last_hidden_states, dim=1).squeeze()\n","        return sentence_embedding.numpy()\n","\n","class ELMoEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__()\n","        options_file = 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json'\n","        weight_file = 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'\n","        self.elmo = Elmo(options_file, weight_file, 1, dropout=0)\n","        self.classifier = LogisticRegression(max_iter=1000)  # Add this line\n","\n","    def train(self, df):\n","        start_time = time.time()\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding_elmo(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","\n","        # Split the dataset into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            list(df['embedding']), df['ChatGPT'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        start_time = time.time()\n","        self.classifier.fit(X_train, y_train)\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test = df.apply(lambda row: self.sentence_embedding_elmo(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        y_pred = self.classifier.predict(list(X_test))\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","\n","\n","        # Evaluate the model\n","        accuracy = accuracy_score(df['ChatGPT'], y_pred)\n","        self.accuracy_percent = accuracy * 100\n","\n","        print('ELMO')\n","        print(f'Accuracy (ELMo): {self.accuracy_percent:.2f}%')\n","        print(f'Embedding Time (ELMo): {self.embedding_time:.2f} seconds')\n","        print(f'Training Time (ELMo): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time (ELMo): {self.prediction_time:.2f} seconds')\n","        print()\n","\n","    def sentence_embedding_elmo(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        character_ids = batch_to_ids([sentence.split()])  # assuming a single sentence\n","        embeddings = self.elmo(character_ids)\n","        sentence_embedding = torch.mean(embeddings['elmo_representations'][0], dim=1).squeeze()\n","        return sentence_embedding.detach().numpy()\n","\n","class USEEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__()\n","        self.use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n","        self.classifier = LogisticRegression(max_iter=1000)\n","\n","    def train(self, df):\n","        start_time = time.time()\n","        df['embedding'] = df.apply(lambda row: self.sentence_embedding_use(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        end_time = time.time()\n","        self.embedding_time = end_time - start_time\n","\n","        # Split the dataset into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            list(df['embedding']), df['ChatGPT'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set\n","        start_time = time.time()\n","        self.classifier.fit(X_train, y_train)\n","        end_time = time.time()\n","        self.training_time = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test = df.apply(lambda row: self.sentence_embedding_use(row['Task name'], row['Type'], row['Intensity']), axis=1)\n","        y_pred = self.classifier.predict(list(X_test))\n","        end_time = time.time()\n","        self.prediction_time = end_time - start_time\n","\n","        # Evaluate the model\n","        accuracy = accuracy_score(df['ChatGPT'], y_pred)\n","        self.accuracy_percent = accuracy * 100\n","        print('USE')\n","        print(f'Accuracy (USE): {self.accuracy_percent:.2f}%')\n","        print(f'Embedding Time (USE): {self.embedding_time:.2f} seconds')\n","        print(f'Training Time (USE): {self.training_time:.2f} seconds')\n","        print(f'Prediction Time (USE): {self.prediction_time:.2f} seconds')\n","        print( )\n","    def sentence_embedding_use(self, task_name, task_type, task_intensity):\n","        sentence = f\"{task_name} {task_type} {task_intensity}\"\n","        embedding = self.use_model([sentence])[0].numpy()\n","        return embedding\n","\n","\n","class Doc2VecEmbeddingModel(EmbeddingModel):\n","    def __init__(self):\n","        super().__init__()\n","        self.doc2vec_model = Doc2Vec(vector_size=300, window=5, min_count=1, workers=4, epochs=20)\n","        self.classifier = LogisticRegression(max_iter=1000)\n","        self.embedding_time_doc2vec = 0\n","        self.training_time_doc2vec = 0\n","        self.prediction_time_doc2vec = 0\n","        self.accuracy_percent_doc2vec = 0\n","\n","    def preprocess_text(self, text):\n","        return word_tokenize(text.lower())\n","\n","    def train(self, df):\n","        # Tokenize and preprocess text for Doc2Vec\n","        df['tokenized_text'] = df.apply(lambda row: self.preprocess_text(row['Task name'] + ' ' + row['Type'] + ' ' + row['Intensity']), axis=1)\n","\n","        start_time = time.time()\n","        documents = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(df['tokenized_text'])]\n","        self.doc2vec_model.build_vocab(documents)\n","        self.doc2vec_model.train(documents, total_examples=self.doc2vec_model.corpus_count, epochs=self.doc2vec_model.epochs)\n","\n","        # Create document embeddings for the entire dataset using Doc2Vec\n","        df['embedding_doc2vec'] = df['tokenized_text'].apply(lambda x: self.doc2vec_model.infer_vector(x))\n","        end_time = time.time()\n","        self.embedding_time_doc2vec = end_time - start_time\n","\n","        # Split the dataset into training and testing sets for Doc2Vec\n","        X_train_doc2vec, X_test_doc2vec, y_train_doc2vec, y_test_doc2vec = train_test_split(\n","            list(df['embedding_doc2vec']), df['ChatGPT'], test_size=0.2, random_state=42\n","        )\n","\n","        # Train a classifier (e.g., Logistic Regression) on the training set for Doc2Vec\n","        start_time = time.time()\n","        self.classifier.fit(X_train_doc2vec, y_train_doc2vec)\n","        end_time = time.time()\n","        self.training_time_doc2vec = end_time - start_time\n","        self.trained = True\n","\n","    def predict(self, df):\n","        if not self.trained:\n","            raise ValueError(\"Model must be trained before making predictions\")\n","\n","        start_time = time.time()\n","        X_test_doc2vec = df['tokenized_text'].apply(lambda x: self.doc2vec_model.infer_vector(x))\n","        y_pred_doc2vec = self.classifier.predict(list(X_test_doc2vec))\n","        end_time = time.time()\n","        self.prediction_time_doc2vec = end_time - start_time\n","\n","        # Compute Accuracy\n","        accuracy_doc2vec = accuracy_score(df['ChatGPT'], y_pred_doc2vec)\n","        self.accuracy_percent_doc2vec = accuracy_doc2vec * 100\n","\n","\n","        print('Doc2vec')\n","        print(f'Accuracy (Doc2Vec): {self.accuracy_percent_doc2vec:.2f}%')\n","        print(f'Embedding Time (Doc2Vec): {self.embedding_time_doc2vec:.2f} seconds')\n","        print(f'Training Time (Doc2Vec): {self.training_time_doc2vec:.2f} seconds')\n","        print(f'Prediction Time (Doc2Vec): {self.prediction_time_doc2vec:.2f} seconds')\n","        print()\n","# Load your CSV dataset\n","df = pd.read_csv('/content/drive/My Drive/Sem-4/Essence/dataset-20240111.csv')\n","\n","# Instantiate models\n","bert_model = BERTEmbeddingModel()\n","elmo_model = ELMoEmbeddingModel()\n","use_model = USEEmbeddingModel()\n","doc2vec_model = Doc2VecEmbeddingModel()\n","\n","# Train models\n","bert_model.train(df)\n","elmo_model.train(df)\n","use_model.train(df)\n","doc2vec_model.train(df)\n","\n","# Make predictions\n","bert_model.predict(df)\n","elmo_model.predict(df)\n","use_model.predict(df)\n","doc2vec_model.predict(df)\n"]},{"cell_type":"markdown","metadata":{"id":"YvaFI3pAV6w5"},"source":["Code today modification 2"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyPAzpBGeyWG5mMHxLRtqW2T"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"05d7d6236a854f52abd92413616f67da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58689c648b3e4f579560504ecf34cb23","placeholder":"​","style":"IPY_MODEL_57eaf7d0b8914adaa79c86939bc627e4","value":"config.json: 100%"}},"06093853f44f4b018ac428749519aa5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_60cdf0c1e4e8464ea70fbffeda452b6f","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9e6a08eb48ed4eabb8e188f5f680d581","value":466062}},"064a0f95211548abb67de2008231c18a":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_d07442a225e34031aa02637f03d69623","msg_id":"","outputs":[{"data":{"text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Downloading <span style=\"color: #008080; text-decoration-color: #008080; font-style: italic\">https://allennlp.s3.amazonaws.com/models/elmo/2x4…</span> <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">  0%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span> <span style=\"color: #008000; text-decoration-color: #008000\">0/336 bytes</span>\n</pre>\n","text/plain":"Downloading \u001b[3;36mhttps://allennlp.s3.amazonaws.com/models/elmo/2x4…\u001b[0m \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[32m0/336 bytes\u001b[0m\n"},"metadata":{},"output_type":"display_data"}]}},"138fcbcf0c8a488490c357e66eca7c46":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_87a726e6a6524e57b40e65e524715389","IPY_MODEL_f3ed01ce39034017a318a9c87138dc0e","IPY_MODEL_748d499d3538475d9c9f8c19e0d370ea"],"layout":"IPY_MODEL_d54514eebfc24750bb317db82300a3fb"}},"14f2e0d4d76346ce91ee65c0a4c16340":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2158db8eff8d4ba9888c22bf2972b5a3","placeholder":"​","style":"IPY_MODEL_d430c842c9cc4bc29cf7cdb1799fe2aa","value":"model.safetensors: 100%"}},"1b256886670d4591a731a4f5cb0b786e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b741ffd2f6a403f997a8a96b8ad2708","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5c09c69a6a514cb48ab4f6e21478e72f","value":48}},"203d333f5f2f40b48748590e505c0d4f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2158db8eff8d4ba9888c22bf2972b5a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26eede88f67f43fa8eb9d557bcc1164b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2931072c6be4626b472b9fc503bf228","placeholder":"​","style":"IPY_MODEL_77bd4f6dd4004b1b9e16e9fe8a845c52","value":"tokenizer_config.json: 100%"}},"2c14bf77ce6f4d40a1b5aa607986d824":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_445adc0b827844d293df108e8c9935b8","msg_id":"","outputs":[{"data":{"text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Downloading <span style=\"color: #008080; text-decoration-color: #008080; font-style: italic\">https://allennlp.s3.amazonaws.com/models/elmo/2x4…</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:10</span> <span style=\"color: #008000; text-decoration-color: #008000\">373.2/374.4 MB</span>\n</pre>\n","text/plain":"Downloading \u001b[3;36mhttps://allennlp.s3.amazonaws.com/models/elmo/2x4…\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:10\u001b[0m \u001b[32m373.2/374.4 MB\u001b[0m\n"},"metadata":{},"output_type":"display_data"}]}},"3563cb17af1e4763b80beb3312a67fe7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_203d333f5f2f40b48748590e505c0d4f","placeholder":"​","style":"IPY_MODEL_934128c214ed41ca8095beb4251b4c5a","value":" 466k/466k [00:00&lt;00:00, 18.0MB/s]"}},"36edc281b9554008b98f9097698061a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"445adc0b827844d293df108e8c9935b8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48884da94e644179bf47570d3899dedf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49fb6e2aed004cb599f01f517b8eb1ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5796444343bd4269865a7a009b9d697c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"57eaf7d0b8914adaa79c86939bc627e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58689c648b3e4f579560504ecf34cb23":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"595026cc9ecc4f88bec16fab47d8e631":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b28750eb55242f7bde2e6bb4775d374":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c09c69a6a514cb48ab4f6e21478e72f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"60cdf0c1e4e8464ea70fbffeda452b6f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63b041de84e243c1a505c567c8c22a80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a7b3db7e4e5a40ca852759e3b1124f5e","IPY_MODEL_06093853f44f4b018ac428749519aa5a","IPY_MODEL_3563cb17af1e4763b80beb3312a67fe7"],"layout":"IPY_MODEL_935f33b7996d468e93f35a4685ffc67c"}},"6b741ffd2f6a403f997a8a96b8ad2708":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bf5464ab7284464a09c315376ebb799":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_595026cc9ecc4f88bec16fab47d8e631","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5796444343bd4269865a7a009b9d697c","value":570}},"6f1042cfd1124157bd73dfd1be66d99d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74309ede0fbc4c27afc26f2d0fc84707":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26eede88f67f43fa8eb9d557bcc1164b","IPY_MODEL_1b256886670d4591a731a4f5cb0b786e","IPY_MODEL_d13afc2439074a919103f3f8f790e9e1"],"layout":"IPY_MODEL_95f9678ab62f4166b049671158439b1b"}},"748d499d3538475d9c9f8c19e0d370ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a95b323589214d87be6c5c672b4c6104","placeholder":"​","style":"IPY_MODEL_8f9fa2000111488bb072daa77dd2b29a","value":" 232k/232k [00:00&lt;00:00, 8.82MB/s]"}},"77bd4f6dd4004b1b9e16e9fe8a845c52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d24c4910da54c14a5d06bde7bff1633":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84429f2c87d342d59c37443a8fef535f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87a726e6a6524e57b40e65e524715389":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7ee32ed2eaa443b9bc86185bdc2ae0e","placeholder":"​","style":"IPY_MODEL_36edc281b9554008b98f9097698061a1","value":"vocab.txt: 100%"}},"8f9fa2000111488bb072daa77dd2b29a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"910a1538c5b64878909664d99dae947f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84429f2c87d342d59c37443a8fef535f","placeholder":"​","style":"IPY_MODEL_49fb6e2aed004cb599f01f517b8eb1ed","value":" 440M/440M [00:02&lt;00:00, 157MB/s]"}},"920c8755cef9445fa98336ac7bdafc78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"934128c214ed41ca8095beb4251b4c5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"935f33b7996d468e93f35a4685ffc67c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95f9678ab62f4166b049671158439b1b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e6a08eb48ed4eabb8e188f5f680d581":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a7b3db7e4e5a40ca852759e3b1124f5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f1042cfd1124157bd73dfd1be66d99d","placeholder":"​","style":"IPY_MODEL_920c8755cef9445fa98336ac7bdafc78","value":"tokenizer.json: 100%"}},"a95b323589214d87be6c5c672b4c6104":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1b65d7bcd204ac9b1d682b90c543103":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_05d7d6236a854f52abd92413616f67da","IPY_MODEL_6bf5464ab7284464a09c315376ebb799","IPY_MODEL_c2288e65f4e0420ebfd1d3229c9609c2"],"layout":"IPY_MODEL_5b28750eb55242f7bde2e6bb4775d374"}},"bf39865316fe42978e6a19745f082a2a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2288e65f4e0420ebfd1d3229c9609c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7c4c50cc5e24270962478bb06e01d9a","placeholder":"​","style":"IPY_MODEL_7d24c4910da54c14a5d06bde7bff1633","value":" 570/570 [00:00&lt;00:00, 18.9kB/s]"}},"c324b874ffd74ed69dcbac524e5dd484":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ca62a0e7b3c744c5b1b9383a79cd109c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d679df0471604c2cb1e49b846a77c3fd","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e702339baf01436e8d1c82a39d4b33c2","value":440449768}},"cb066456d7384fa7a0048633b9af9d45":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdbd3ed698fc4b8dbfa675731bb1a0d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_14f2e0d4d76346ce91ee65c0a4c16340","IPY_MODEL_ca62a0e7b3c744c5b1b9383a79cd109c","IPY_MODEL_910a1538c5b64878909664d99dae947f"],"layout":"IPY_MODEL_48884da94e644179bf47570d3899dedf"}},"d07442a225e34031aa02637f03d69623":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d13afc2439074a919103f3f8f790e9e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb066456d7384fa7a0048633b9af9d45","placeholder":"​","style":"IPY_MODEL_d4a63a52f517429ca505dba0e3c2c8fb","value":" 48.0/48.0 [00:00&lt;00:00, 2.80kB/s]"}},"d430c842c9cc4bc29cf7cdb1799fe2aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4a63a52f517429ca505dba0e3c2c8fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d54514eebfc24750bb317db82300a3fb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d679df0471604c2cb1e49b846a77c3fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2931072c6be4626b472b9fc503bf228":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e702339baf01436e8d1c82a39d4b33c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e7ee32ed2eaa443b9bc86185bdc2ae0e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3ed01ce39034017a318a9c87138dc0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf39865316fe42978e6a19745f082a2a","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c324b874ffd74ed69dcbac524e5dd484","value":231508}},"f7c4c50cc5e24270962478bb06e01d9a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}